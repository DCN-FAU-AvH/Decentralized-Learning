The codes simulates a classification task over the MNIST-digits dataset, consisting of grayscale images of handwritten digits from 0 to 9.

We explore both statistically homogeneous (IID) and heterogeneous (non-IID) data distributions among agents. Our goal is to understand how the scalarization parameter, which balances local and global objectives, influences learning dynamics, convergence behavior, and generalization accuracy.

For more details about the experiments, please read Section 3 (Experimental evaluation) in the paper https://arxiv.org/abs/2507.13983  

Both codes are written in a python script .ipynb. Each code can be launched directly (for example) by using Anaconda. Please, make sure to create a kernel compatible with pytorch, sklearn, numpy, etc.

 
